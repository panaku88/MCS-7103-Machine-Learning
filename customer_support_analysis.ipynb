{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panaku88/MCS-7103-Machine-Learning/blob/main/customer_support_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auBISEL2nmJx"
      },
      "source": [
        "Based on last week's lecture, your assignment is to perform Exploratory Data Analysis Process (DAP) on your dataset and write a report that is at least 3 pages long. You can write as many pages as you need. The report needs to be clear and follow a step-by-step process.\n",
        "1.\tPresent your question before and after, then the answers. The answers can be before the next step (Data Wrangling), every process or after the full process. The question that helped you select the dataset should differ from the questions you are using for the rest of the process. I have general questions to guide you through your analysis.\n",
        "2.\tPerform in-depth data wrangling.\n",
        "3.\tProvide a well-detailed Exploratory Data Analysis (EDA).\n",
        "4.\tDraw some conclusions based on the performed EDA.\n",
        "5.\tCommunicate the ﬁndings in a detailed report.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INTRODUCTION**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Customer satisfaction is an import growth component for any business and in this task, my main objective was to analyze the different touch points between the customer and the different departments of SONIC Company LTD. Such engagements include presales activities/data, technical assistance data, customers care data, and billing queries to find patterns and relationships for improving overall customer services and satisfaction.\n",
        "What can be done to improve customer satisfaction?\n",
        "\n",
        "1.   What can be done to improve customer satisfaction?\n",
        "2.   What kind of information do I need to achieve this?\n",
        "3.   What processes currently exist?\n",
        "3.   How do customers reach out to us?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WYC92qH0UNV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA COLLECTION**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The dataset was extracted from the customer management system of SONIC Co. LTD. This includes all logged interactions (queries or complaints) between the customer and sales team, customer and customer care team, customer and billing team, customer and the field support team. The period under consideration is January 2020 to December 2023. Below are the data sources in the dataset.\n",
        "\n",
        "\n",
        "\n",
        "1.   Presales data – captured presales data include customer requirements\n",
        "2.   Technical support data – interactions between customers and technical team.\n",
        "3.   Customer care team – interactions between customer support team members while following up on issues.\n",
        "4.   Billing queries - records relating to customer billing issues.\n",
        "After collecting the data, below are the questions that I asked myself to help me in the data analysis\n",
        "5.   How do we improve customer services?\n",
        "6.   How do we measure all the customer touch points.\n",
        "7.   What insight can I learn from the data captured.\n",
        "8.   What are the most common issues reported by customers?\n",
        "\n"
      ],
      "metadata": {
        "id": "HTyR8XmNUc8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA WRANGLING**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Described in this section are the data pre-processing approach that I used to clean and organize the data. Python was chosen as the data analysis tool.\n",
        "Important Python libraries: The first step in the data wrangling involved importing the python libraries Pandas, Matplotlib, Seaborn. Pandas is an open-source python data analysis and manipulation tool, Matplotlib is used to perform data visualization in python and lastly seaborn is also a data visualization tool (based on Matplotlib) and is closed integrated with the Pandas data structures making it easier to use, with beautiful statistical graphs.\n",
        "Reading the Dataset: After extracting the raw customer support dataset, which was extracted in the CSV file format, I loaded the dataset into a Pandas Dataframe. The dataset was downloaded and named as a csv file and then loaded into Pandas data frame for cleaning and exploratory analysis.\n",
        "Assessing the Dataset: the purpose of this phase was to mainly dig into the dataset and understand the structure, content and check if there are problems in the dataset. The detailed process involved looking at the features attributes in the dataset for instance the Pandas functions head()and dtypes() were useful in understanding the meaning of and datatype of each column in my dataset.\n",
        "Dataset Cleaning: After understanding the structure and content of the dataset I realized that it was necessary to clean the data as there were some missing values and some attributes that were not important for my purpose. The process involved writing some python code that checked for any null value values and removed them, renamed certain fields as this dataset contains sensitive information.\n"
      ],
      "metadata": {
        "id": "sj-VtFDfWdld"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs6PFotHORQh"
      },
      "source": [
        "1. Here I am importing the Google Drive Python Library which is used to connect to colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFStSYFVq4oF",
        "outputId": "643e184d-7ca7-4c0e-abc4-ca738bf4daa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21vvsdUaOrZl"
      },
      "source": [
        "2. Importing the necessary Python Libraries discussed in the report write up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onf44ur0Clid"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import re\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoLjQm8MO59d"
      },
      "source": [
        "3. Reading the raw customer dataset into a Pandas Dataframe and specifying a custom directory (output_path) to store modified dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZtrVGe9ylRD"
      },
      "outputs": [],
      "source": [
        "raw_dataset = pd.read_csv('/content/drive/MyDrive/MCSC1/dataset/CS_Service_Data.csv')\n",
        "\n",
        "# Specify the path to save the modified/manipulated dataset\n",
        "output_path = '/content/drive/MyDrive/MCSC1/dataset/customer_support_dataset.csv'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU_SMQA5PoBD"
      },
      "source": [
        "4. Here next code cells, I am assessing the Dataset in order to understand the structure, content and check if there are problems in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzPF1cVVNHU9"
      },
      "outputs": [],
      "source": [
        "raw_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset.dtypes"
      ],
      "metadata": {
        "id": "LnO6FYzba9s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQhvhzb5NQNz"
      },
      "outputs": [],
      "source": [
        "raw_dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset.shape"
      ],
      "metadata": {
        "id": "4JTNNkpZaMnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset.columns"
      ],
      "metadata": {
        "id": "pffbsD8gaSv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset.nunique()"
      ],
      "metadata": {
        "id": "guYjok9CbJyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxE6fPH_NRee"
      },
      "outputs": [],
      "source": [
        "raw_dataset.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyDMRG30NT--"
      },
      "outputs": [],
      "source": [
        "raw_dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWoYmSmZQYhT"
      },
      "source": [
        "5. The following are custom Python code to manipulate the dataset. Precisely, we are removing any sensitive information from the data and any missing or null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZelpEMYsVCm"
      },
      "outputs": [],
      "source": [
        "customer_names = {}\n",
        "syllables = ['a', 'e', 'i', 'o', 'u', 'ka', 'ko', 'sa', 'tu', 'ma', 'me', 'mi', 'mo', 'mu', 'ya', 'ye', 'yi', 'yo', 'yu', 'ra', 're', 'ri', 'ro', 'ru', 'wa', 'we', 'wi', 'wo', 'wu']\n",
        "\n",
        "def generate_name(min_length=3, max_length=6):\n",
        "  name = ''\n",
        "  length = random.randint(min_length, max_length)\n",
        "  for i in range(length):\n",
        "    name += random.choice(syllables)\n",
        "  return name.capitalize()\n",
        "\n",
        "def generate_customer_name(row):\n",
        "  name = row['CUSTOMER NAME']\n",
        "  account = row['CUSTOMER ACCOUNT']\n",
        "  if isinstance(name, str):\n",
        "    if account in customer_names:\n",
        "      return customer_names[account]\n",
        "    else:\n",
        "      if 'Mr' in name or 'Ms' in name:\n",
        "        title = random.choice(['Mr', 'Ms'])\n",
        "        first_name = generate_name()\n",
        "        last_name = generate_name()\n",
        "        new_name = f'{title} {first_name} {last_name}'\n",
        "      elif 'Company' in name or 'Ltd' in name or 'Inc' in name:\n",
        "        new_name = generate_name() + ' Inc'\n",
        "      else:\n",
        "        first_name = generate_name()\n",
        "        last_name = generate_name()\n",
        "        new_name = f'{first_name} {last_name}'\n",
        "      customer_names[account] = new_name\n",
        "      return new_name\n",
        "  else:\n",
        "    return name\n",
        "\n",
        "raw_dataset['CUSTOMER NAME'] = raw_dataset.apply(generate_customer_name, axis=1)\n",
        "\n",
        "\n",
        "# Define a function to replace senstive user attributes\n",
        "def replace_rke_with_tt(text):\n",
        "    # Check if the value is a string\n",
        "    if isinstance(text, str):\n",
        "        # Here i am modifying account information\n",
        "        return re.sub(r'\\bRKE(\\w*)\\b', r'TT\\1', text)\n",
        "    else:\n",
        "        # Return the original value if it's not a string\n",
        "        return text\n",
        "\n",
        "# Here i am replacing actual service plans\n",
        "def replace_service_plan(text):\n",
        "  if isinstance(text, str):\n",
        "    if re.match(r'CAPPED-BASE: Roke Capped Base', text):\n",
        "      return 'SONIC HOME PRO 25Mbps'\n",
        "    else:\n",
        "      return text\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "# Here i am modifying another service plan\n",
        "def replace_service_plan_ent(text):\n",
        "  if isinstance(text, str):\n",
        "    match = re.search(r'RE(\\d+): Roke Enterprise', text)\n",
        "    if match:\n",
        "      number = match.group(1)\n",
        "      return f'SONIC BUSINESS {number}Mbps'\n",
        "    else:\n",
        "      return text\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "\n",
        "def replace_service_plan_vpn(text):\n",
        "  if isinstance(text, str):\n",
        "    if 'VPN' in text or 'vpn' in text:\n",
        "      random_number = random.randint(1, 100)\n",
        "      return f'SONIC MPLS VPN {random_number}Mbps'\n",
        "    else:\n",
        "      return text\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "def replace_csnoc(text):\n",
        "  if isinstance(text, str):\n",
        "    if text == 'CSNOC':\n",
        "      return 'SONIC SUPPORT'\n",
        "    else:\n",
        "      return text\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "# here i am calling the above functions to manipulate the dataset\n",
        "raw_dataset['SERVICE PLAN'] = raw_dataset['SERVICE PLAN'].apply(replace_service_plan_vpn)\n",
        "raw_dataset['SERVICE PLAN'] = raw_dataset['SERVICE PLAN'].apply(replace_service_plan_ent)\n",
        "raw_dataset['SERVICE PLAN'] = raw_dataset['SERVICE PLAN'].apply(replace_service_plan)\n",
        "raw_dataset['TICKET NUMBER'] = raw_dataset['TICKET NUMBER'].str.replace(r'^RKE', 'TT', regex=True)\n",
        "raw_dataset['TICKET CENTER'] = raw_dataset['TICKET CENTER'].apply(replace_csnoc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcbH9JRhRpyL"
      },
      "source": [
        "6. Data cleanning: After understanding the structure and content in step 5 I realized that it was necessary to clean the data as there were some missing values and some attributes that were not important for my purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuiH_lB3-Ofk"
      },
      "outputs": [],
      "source": [
        "# Save the modified dataset to the specified path\n",
        "raw_dataset.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"The clean_cs_dataset.csv has been created successfully at {output_path}.\")\n",
        "new_dataset = pd.read_csv('/content/drive/MyDrive/MCSC1/dataset/customer_support_dataset.csv')\n",
        "\n",
        "# Drop rows with 'Not Specified' or 'Not Selected'.\n",
        "new_dataset = new_dataset[new_dataset.applymap(lambda x: 'Not Specified' not in\n",
        "                                               str(x) and 'Not Selected' not in str(x) and 'Shared Bandwidth' not in str(x)).all(axis=1)]\n",
        "\n",
        "# Drop the unnecessary attributes\n",
        "# new_dataset = new_dataset.dropna(subset=['TICKET OWNER'])\n",
        "# new_dataset = new_dataset.dropna(subset=['SOURCE'])\n",
        "new_dataset = new_dataset.dropna(subset=['CATEGORY', 'SUB-CATEGORY', 'SOURCE', 'TICKET OWNER'])\n",
        "new_dataset = new_dataset.drop('CURRENT STATUS', axis=1)\n",
        "new_dataset = new_dataset.drop('ALLOCATED TIME (HOURS)', axis=1)\n",
        "new_dataset = new_dataset.drop('LOGGED DESCRIPTION', axis=1)\n",
        "new_dataset = new_dataset.drop('TICKET OWNER', axis=1)\n",
        "new_dataset = new_dataset.drop('Logged By User', axis=1)\n",
        "new_dataset = new_dataset.drop('CUSTOMER ACCOUNT', axis=1)\n",
        "\n",
        "# Create a dictionary to map source categories into numerical values\n",
        "source_mapping = {'Email': 1, 'Live Chat': 2, 'Phone': 3, 'Internal Process': 4,\n",
        "                  'SMS': 5, 'Fax': 6, 'Letter': 7, 'Socials': 8}\n",
        "new_dataset['SOURCE'] = new_dataset['SOURCE'].map(source_mapping)\n",
        "\n",
        "# Save the modified dataset to the specified path\n",
        "new_dataset.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2ufLX72tgtC"
      },
      "outputs": [],
      "source": [
        "new_dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_dataset.shape"
      ],
      "metadata": {
        "id": "n1IXYvazasBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_dataset.columns"
      ],
      "metadata": {
        "id": "1bvu87Anau5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Hi3hIfzN0TQ"
      },
      "outputs": [],
      "source": [
        "new_dataset.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sTv48SgtwBK"
      },
      "outputs": [],
      "source": [
        "new_dataset.tail(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtpbBIvTtyiT"
      },
      "outputs": [],
      "source": [
        "new_dataset.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17z8GC7Bt1eD"
      },
      "outputs": [],
      "source": [
        "new_dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGJ18BfLuOBc"
      },
      "outputs": [],
      "source": [
        "new_dataset.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gpTkyz2uT5J"
      },
      "outputs": [],
      "source": [
        "new_dataset.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou1bwRnFucFO"
      },
      "outputs": [],
      "source": [
        "new_dataset.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPLORATORY DATA ANALYSIS**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "After performing the data assessment part, I was able to identify certain features that were imported for my purpose.\n",
        "Some of the important features I identified from the dataset were, the complaint or query category, the time was logged and resolved, service plan of customer, incident id, the customer account with SONIC, whether query came via email, phone or self-logged by user\n",
        "The features identified for the analysis are total confirmed cases, total recovered cases, total deaths, continents, and population. The reason for choosing these features is simply because we want to know how COVID-19 has affected various parts of the world by comparing confirmed cases with death rate and how people are recovering using their populations.\n"
      ],
      "metadata": {
        "id": "_md5GlRQXaE2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yuCBv8vZS-1"
      },
      "source": [
        "A Bar plot to investigate the number of incidents logged per service plan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0c_THVjhQj_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(data=new_dataset, x='SUB-CATEGORY')\n",
        "sns.displot(new_dataset, x=\"SUB-CATEGORY\", hue=\"CATEGORY\")\n",
        "plt.title('Customer Complaints by Category')\n",
        "plt.xlabel('Sub-Categories')\n",
        "plt.ylabel('Customer Complaints')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYamnAS-uqZ5"
      },
      "source": [
        "Investigating the relationship between ticket prioroty and resolution time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnH07XEkqAJI"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=new_dataset, x='PRIORITY', y='TIME TO CLOSE (MINS)')\n",
        "plt.title('Relationship between Ticket Priority and Resolution Time')\n",
        "plt.xlabel('Priority')\n",
        "plt.ylabel('Resolution Time (Hours)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A visualization of the number of tickets by ticket source"
      ],
      "metadata": {
        "id": "6BYJGEYln8YA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Invert the source_mapping dictionary\n",
        "inverted_source_mapping = {v: k for k, v in source_mapping.items()}\n",
        "\n",
        "# Create a new column with categorical values for 'SOURCE'\n",
        "new_dataset['SOURCE_CATEGORY'] = new_dataset['SOURCE'].map(inverted_source_mapping)\n",
        "\n",
        "# Create the countplot with hue set to the new column\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=new_dataset, x='SOURCE', hue='SOURCE_CATEGORY')\n",
        "plt.title('Distribution of Ticket Sources')\n",
        "plt.xlabel('Ticket Source')\n",
        "plt.ylabel('Number of Tickets')\n",
        "plt.legend(title='Ticket Source')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "clvpiCoQn0KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I am exploring the relationship between ticket source and resolution time of the tickets using a bax plot"
      ],
      "metadata": {
        "id": "wt4CQKnWpgjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create the boxplot without a legend\n",
        "sns.boxplot(data=new_dataset, x='SOURCE_CATEGORY', y='TIME TO CLOSE (MINS)', showfliers=False)\n",
        "\n",
        "# Create a custom legend\n",
        "handles = [plt.Rectangle((0, 0), 1, 1, fc=\"white\", ec=\"black\")] * len(new_dataset['SOURCE_CATEGORY'].unique())\n",
        "labels = new_dataset['SOURCE_CATEGORY'].unique()\n",
        "plt.legend(handles, labels, title='Ticket Source')\n",
        "\n",
        "plt.title('Relationship between Ticket Source and Resolution Time')\n",
        "plt.xlabel('Ticket Source')\n",
        "plt.ylabel('Resolution Time (Minutes)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C1HW3vtypfqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, explored the relationship between numner of tickets and ticket centers"
      ],
      "metadata": {
        "id": "tGnIAwBts4uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=new_dataset, x='TICKET CENTER')\n",
        "plt.title('Number of Tickets per Ticket Center')\n",
        "plt.xlabel('Ticket Center')\n",
        "plt.ylabel('Number of Tickets')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sJlmigzls31q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUvwghGgrGIs"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(data=new_dataset, x='SERVICE PLAN', y='TIME TO CLOSE (MINS)', errorbar=None)\n",
        "#sns.displot(new_dataset, x=\"TIME TO CLOSE (MINS)\", hue=\"SERVICE PLAN\")\n",
        "plt.title('Average Resolution Time for Each Service Plan')\n",
        "plt.xlabel('Service Plan')\n",
        "plt.ylabel('Average Resolution Time (Hours)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1k2ULLPqYd5sanaOODkz0WPl2wPYz1Vha",
      "authorship_tag": "ABX9TyNj/YqqhqyZ1wQBMg7Ae5UN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}